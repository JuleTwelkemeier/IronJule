{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We practiced web scraping when all the information is in a single table of a single page in a site. What happens when we want to scrape information from multiple pages?\n",
    "\n",
    "Go to https://www.imdb.com/search/title/ and enter the following parameters, leaving all other fields blank or with its default value:\n",
    "\n",
    "- Title Type: Feature film\n",
    "\n",
    "- Release date: From 1990 to 1992\n",
    "\n",
    "- User Rating: 7.5 to \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he page you get should be familiar. There's a list with movies and each movie has its title, release year, crew, etc. You could inspect the page and build the code to collect the date.\n",
    "\n",
    "However, the results we obtained contain 631 movies, and each page only contains 50 of them (you can change the settings to obtain up to 250 movies/page, but that still won't make it till the end).\n",
    "\n",
    "The way to automatize web scraping in these cases is to look at the URLs The one we've obtained is the following:\n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,\n",
    "\n",
    "If you scroll down and click on \"Next\", the URL is now: https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
    "\n",
    "Click again on \"Next\" and here's the new URL: https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
    "\n",
    "The patterns are clear: our search options are in the parameters title_type, release_dateand user_rating. Then, we have the start parameter, which jumps in intervals of 50, and the ref_ parameter, which takes the value of \"adv_nxt\".\n",
    "\n",
    "Let's do some requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. url: we start with the 'second' page\n",
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# 4.2. check that the html code looks like it should\n",
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll have to build a loop where we simply replace the 51 for all the other values (jumping by 50) up until the end of the results. For simplicity, we will build manually this list of values to iterate through:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=1&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=51&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=101&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=151&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=201&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=251&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=301&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=351&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=401&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=451&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=501&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=551&ref_=adv_nxt\n",
      "https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=601&ref_=adv_nxt\n"
     ]
    }
   ],
   "source": [
    "iterations = range(1, 631, 50)\n",
    "\n",
    "for i in iterations:\n",
    "    start_at= str(i)\n",
    "    url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=\" + start_at + \"&ref_=adv_nxt\"\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Respectful scraping:\n",
    "\n",
    "Before starting with the actual scraping, though, there's something we need to note when sending massive, automated requests to websites: it's rude.\n",
    "\n",
    "We just have 13 of them, which is not too many, but it's still a good practice to let a few seconds pass in between requests. Some pages don't like being scraped and will block your IP if they detect it's sending automated requests. Others might have a small server for the traffic they handle, and sending too many requests might crash the site.\n",
    "\n",
    "The sleep module will help us with that. Here's how it works, waiting 2 seconds between each iteration in a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "I will sleep for 4 seconds.\n",
      "1\n",
      "I will sleep for 4 seconds.\n",
      "2\n",
      "I will sleep for 4 seconds.\n",
      "3\n",
      "I will sleep for 4 seconds.\n",
      "4\n",
      "I will sleep for 2 seconds.\n"
     ]
    }
   ],
   "source": [
    "# To make it more \"human\", we can randomize the waiting time:\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    wait_time = randint(1,4)\n",
    "    print(\"I will sleep for \" + str(wait_time) + \" seconds.\")\n",
    "    sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now scrape all the pages and store the response into a list - waiting a few seconds in between requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "I will sleep for 1 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 3 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 1 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 4 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 4 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 3 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 3 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 4 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 2 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 3 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 4 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 4 second/s.\n",
      "Status code: 200\n",
      "I will sleep for 2 second/s.\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "\n",
    "for i in iterations:\n",
    "\n",
    "    # assemble the url:\n",
    "    start_at= str(i)\n",
    "    url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=1990-01-01,1992-12-31&user_rating=7.5,&start=\" + start_at + \"&ref_=adv_nxt\"\n",
    "\n",
    "    # download html with a get request:\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # monitor the process by printing the status code\n",
    "    print(\"Status code: \" + str(response.status_code))\n",
    "\n",
    "    # store response into \"pages\" list\n",
    "    pages.append(response)\n",
    "\n",
    "    # respectful nap:\n",
    "    wait_time = randint(1,4)\n",
    "    print(\"I will sleep for \" + str(wait_time) + \" second/s.\")\n",
    "    sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how if you print the object pages after running the code above, you'll just see the response code messages, but the html code is still accessible and you can parse it the same way we've always done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BeautifulSoup(pages[0].content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the moment to build the code that collects all the 631 movie titles and their synopsis in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/title/tt0099685/\">GoodFellas - Drei Jahrzehnte in der Mafia</a>,\n",
       " <a href=\"/title/tt0102926/\">Das Schweigen der Lämmer</a>,\n",
       " <a href=\"/title/tt0105236/\">Reservoir Dogs: Wilde Hunde</a>,\n",
       " <a href=\"/title/tt0101921/\">Grüne Tomaten</a>,\n",
       " <a href=\"/title/tt0105695/\">Erbarmungslos</a>,\n",
       " <a href=\"/title/tt0099487/\">Edward mit den Scherenhänden</a>,\n",
       " <a href=\"/title/tt0103064/\">Terminator 2: Tag der Abrechnung</a>,\n",
       " <a href=\"/title/tt0099785/\">Kevin - Allein zu Haus</a>,\n",
       " <a href=\"/title/tt0104257/\">Eine Frage der Ehre</a>,\n",
       " <a href=\"/title/tt0100802/\">Total Recall - Die totale Erinnerung</a>,\n",
       " <a href=\"/title/tt0099348/\">Der mit dem Wolf tanzt</a>,\n",
       " <a href=\"/title/tt0104691/\">Der letzte Mohikaner</a>,\n",
       " <a href=\"/title/tt0099674/\">Der Pate 3</a>,\n",
       " <a href=\"/title/tt0106308/\">Armee der Finsternis</a>,\n",
       " <a href=\"/title/tt0105323/\">Der Duft der Frauen</a>,\n",
       " <a href=\"/title/tt0099810/\">Jagd auf Roter Oktober</a>,\n",
       " <a href=\"/title/tt0104952/\">Mein Vetter Winnie</a>,\n",
       " <a href=\"/title/tt0101414/\">Die Schöne und das Biest</a>,\n",
       " <a href=\"/title/tt0103639/\">Aladdin</a>,\n",
       " <a href=\"/title/tt0103074/\">Thelma &amp; Louise</a>,\n",
       " <a href=\"/title/tt0099871/\">Jacob's Ladder - In der Gewalt des Jenseits</a>,\n",
       " <a href=\"/title/tt0101507/\">Boyz n the Hood - Jungs im Viertel</a>,\n",
       " <a href=\"/title/tt0100157/\">Misery</a>,\n",
       " <a href=\"/title/tt0102138/\">JFK: Tatort Dallas</a>,\n",
       " <a href=\"/title/tt0104797/\">Malcolm X</a>,\n",
       " <a href=\"/title/tt0101258/\">Days of Being Wild</a>,\n",
       " <a href=\"/title/tt0101889/\">König der Fischer</a>,\n",
       " <a href=\"/title/tt0099077/\">Zeit des Erwachens</a>,\n",
       " <a href=\"/title/tt0104348/\">Glengarry Glen Ross</a>,\n",
       " <a href=\"/title/tt0104684/\">Hard Boiled</a>,\n",
       " <a href=\"/title/tt0103939/\">Chaplin - Das Leben der unsterblichen Filmlegende</a>,\n",
       " <a href=\"/title/tt0103873/\">Braindead</a>,\n",
       " <a href=\"/title/tt0100150/\">Miller's Crossing</a>,\n",
       " <a href=\"/title/tt0105151/\">The Player</a>,\n",
       " <a href=\"/title/tt0101605/\">Die Commitments</a>,\n",
       " <a href=\"/title/tt0101428/\">Die schöne Querulantin</a>,\n",
       " <a href=\"/title/tt0101410/\">Barton Fink</a>,\n",
       " <a href=\"/title/tt0103905/\">Mann beißt Hund</a>,\n",
       " <a href=\"/title/tt0105046/\">Von Mäusen und Menschen</a>,\n",
       " <a href=\"/title/tt0104652/\">Porco Rosso</a>,\n",
       " <a href=\"/title/tt0101765/\">Die zwei Leben der Veronika</a>,\n",
       " <a href=\"/title/tt0101700/\">Delicatessen</a>,\n",
       " <a href=\"/title/tt0101318/\">Die Liebenden von Pont-Neuf</a>,\n",
       " <a href=\"/title/tt0102536/\">Night on Earth</a>,\n",
       " <a href=\"/title/tt0101985/\">Ein Sommer zum Verlieben</a>,\n",
       " <a href=\"/title/tt0099334/\">Cyrano von Bergerac</a>,\n",
       " <a href=\"/title/tt0102587/\">Tränen der Erinnerung</a>,\n",
       " <a href=\"/title/tt0101640/\">Rote Laterne</a>,\n",
       " <a href=\"/title/tt0100234/\">Close Up</a>,\n",
       " <a href=\"/title/tt0100998/\">Akira Kurosawa's Träume</a>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse just the first page, for testing purposes\n",
    "#you need to select the different pages as each of the is an own list item\n",
    "soup = BeautifulSoup(pages[0].content, \"html.parser\")\n",
    "\n",
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select(\"#main > div > div.lister.list.detail.sub-list > div > div:nth-child(1) > div.lister-item-content > h3 > a\")\n",
    "\n",
    "# Trim the selection: now it grabs all the titles\n",
    "soup.select(\"div.lister-item-content > h3 > a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### synopsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text-muted\">\n",
       " The story of <a href=\"/name/nm1453737\">Henry Hill</a> and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.</p>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paste the Selector from the first movie title copied from Chrome Dev Tools\n",
    "soup.select(\"#main > div > div.lister.list.detail.sub-list > div > div:nth-child(1) > div.lister-item-content > p:nth-child(4)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"text-muted\">\n",
       " The story of <a href=\"/name/nm1453737\">Henry Hill</a> and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A young F.B.I. cadet must receive the help of an incarcerated and manipulative cannibal killer to help catch another serial killer, a madman who skins his victims.</p>,\n",
       " <p class=\"text-muted\">\n",
       " When a simple jewelry heist goes horribly wrong, the surviving criminals begin to suspect that one of them is a police informant.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A housewife who is unhappy with her life befriends an old lady in a nursing home and is enthralled by the tales she tells of people she used to know.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Retired Old West gunslinger William Munny reluctantly takes on one last job, with the help of his old partner Ned Logan and a young man, The \"Schofield Kid.\"</p>,\n",
       " <p class=\"text-muted\">\n",
       " An artificial man, who was incompletely constructed and has scissors for hands, leads a solitary life. Then one day, a suburban lady meets him and introduces him to her world.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A cyborg, identical to the one who failed to kill Sarah Connor, must now protect her ten year old son, John Connor, from a more advanced and powerful cyborg.</p>,\n",
       " <p class=\"text-muted\">\n",
       " An eight-year-old troublemaker must protect his house from a pair of burglars when he is accidentally left home alone by his family during Christmas vacation.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Military lawyer Lieutenant Daniel Kaffee defends Marines accused of murder. They contend they were acting under orders.</p>,\n",
       " <p class=\"text-muted\">\n",
       " When a man goes in to have virtual vacation memories of the planet Mars implanted in his mind, an unexpected and harrowing series of events forces him to go to the planet for real - or is he?</p>,\n",
       " <p class=\"text-muted\">\n",
       " Lieutenant John Dunbar, assigned to a remote western Civil War outpost, befriends wolves and Indians, making him an intolerable aberration in the military.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Three trappers protect the daughters of a British Colonel in the midst of the French and Indian War.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Follows Michael Corleone, now in his 60s, as he seeks to free his family from crime and find a suitable successor to his empire.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A sardonic hardware store clerk is accidentally transported to 1300 A.D., where he must retrieve the Necronomicon and battle an army of the dead so he can return home.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A prep school student needing money agrees to \"babysit\" a blind man, but the job is not at all what he anticipated.</p>,\n",
       " <p class=\"text-muted\">\n",
       " In November 1984, the Soviet Union's best submarine Captain in their newest sub violates orders and heads for the U.S. Is he trying to defect or to start a war?</p>,\n",
       " <p class=\"text-muted\">\n",
       " Two New Yorkers accused of murder in rural Alabama while on their way back to college call in the help of one of their cousins, a loudmouth lawyer with no trial experience.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A prince cursed to spend his days as a hideous monster sets out to regain his humanity by earning a young woman's love.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A kindhearted street urchin and a power-hungry Grand Vizier vie for a magic lamp that has the power to make their deepest wishes come true.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Two best friends set out on an adventure, but it soon turns around to a terrifying escape from being hunted by the police, as these two girls escape for the crimes they committed.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Mourning his dead child, a haunted Vietnam War veteran attempts to uncover his past while suffering from a severe case of dissociation. To do so, he must decipher reality and life from his own dreams, delusions, and perceptions of death.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Follows the lives of three young males living in the Crenshaw ghetto of Los Angeles, dissecting questions of race, relationships, violence, and future prospects.</p>,\n",
       " <p class=\"text-muted\">\n",
       " After a famous author is rescued from a car crash by a fan of his novels, he comes to realize that the care he is receiving is only the beginning of a nightmare of captivity and abuse.</p>,\n",
       " <p class=\"text-muted\">\n",
       " New Orleans District Attorney <a href=\"/name/nm0308426\">Jim Garrison</a> discovers there's more to the Kennedy assassination than the official story.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Biographical epic of the controversial and influential Black Nationalist leader, from his early life and career as a small-time gangster, to his ministry as a member of the Nation of Islam.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A man tries to find out who his real mother is after the woman who raised him tells him the truth.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A former radio DJ, suicidally despondent because of a terrible mistake he made, finds redemption in helping a deranged homeless man who was an unwitting victim of that mistake.</p>,\n",
       " <p class=\"text-muted\">\n",
       " The victims of an encephalitis epidemic many years ago have been catatonic ever since, but now a new drug offers the prospect of reviving them.</p>,\n",
       " <p class=\"text-muted\">\n",
       " An examination of the machinations behind the scenes at a real estate office.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A tough-as-nails cop teams up with an undercover agent to shut down a sinister mobster and his crew.</p>,\n",
       " <p class=\"text-muted\">\n",
       " An elderly Charlie Chaplin discusses his autobiography with his editor, recounting his amazing journey from his poverty-stricken childhood to world-wide success after the ingenious invention of the Little Tramp.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A young man's mother is bitten by a Sumatran rat-monkey. She gets sick and dies, at which time she comes back to life, killing and eating dogs, nurses, friends, and neighbors.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Tom Reagan, an advisor to a Prohibition-era crime boss, tries to keep the peace between warring mobs but gets caught in divided loyalties.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A Hollywood studio executive is being sent death threats by a writer whose script he rejected, but which one?</p>,\n",
       " <p class=\"text-muted\">\n",
       " Jimmy Rabbitte, an unemployed Dublin boy, decides to put together a soul band made up entirely of the Irish working class.</p>,\n",
       " <p class=\"text-muted\">\n",
       " The former famous painter Frenhofer revisits an abandoned project using the girlfriend of a young visiting artist. Questions about truth, life, and artistic limits are explored.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A renowned New York playwright is enticed to California to write for the movies and discovers the hellish truth of Hollywood.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A film crew follows a ruthless thief and heartless killer as he goes about his daily routine. But complications set in when the film crew lose their objectivity and begin lending a hand.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A nomadic farm worker looks after his dimwitted, gentle-giant friend during the Great Depression.</p>,\n",
       " <p class=\"text-muted\">\n",
       " In 1930s Italy, a veteran World War I pilot is cursed to look like an anthropomorphic pig.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Two parallel stories about two identical women; one living in Poland, the other in France. They don't know each other, but their lives are nevertheless profoundly connected.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Post-apocalyptic surrealist black comedy about the landlord of an apartment building who occasionally prepares a delicacy for his odd tenants.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Alex, who's homeless and addicted to alcohol, and Michèle, who's losing her sight, form a relationship while sleeping rough on Paris's Pont-Neuf bridge.</p>,\n",
       " <p class=\"text-muted\">\n",
       " An anthology of 5 different cab drivers in 5 American and European cities and their remarkable fares on the same eventful night.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Based on a true story, primarily on a conflict between two youth gangs, a 14-year-old boy's girlfriend conflicts with the head of one gang for an unclear reason, until finally the conflict comes to a violent climax.</p>,\n",
       " <p class=\"text-muted\">\n",
       " Famed swordsman and poet Cyrano de Bergerac is in love with his cousin Roxane. He has never expressed his love for her as he his large nose undermines his self-confidence. Then he finds a way to express his love to her, indirectly.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A twenty-seven-year-old office worker travels to the countryside while reminiscing about her childhood in Tokyo.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A young woman becomes the fourth wife of a wealthy lord, and must learn to live with the strict rules and tensions within the household.</p>,\n",
       " <p class=\"text-muted\">\n",
       " The true story of <a href=\"/name/nm0754969\">Hossain Sabzian</a>, a cinephile who impersonated the director <a href=\"/name/nm0538532\">Mohsen Makhmalbaf</a> to convince a family they would star in his so-called new film.</p>,\n",
       " <p class=\"text-muted\">\n",
       " A collection of tales based upon eight of director Akira Kurosawa's recurring dreams.</p>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trim the selection: now it grabs all the titles\n",
    "soup.select(\"div.lister-item-content > p:nth-child(4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many approaches to do this. The one we'll follow is: \n",
    "\n",
    "- Loop through the pages we collected, parse them (\"create the soup\") and store the parsed pages in a list. \n",
    "\n",
    "- For each parsed page, select the \"blocks of HTML elements\" that contain all the information of each movie (the title, the synopsis and other stuff). \n",
    "\n",
    "- For each one of the \"blocks\" we collected in the previous step: \n",
    "\n",
    "    - Get the movie titles and store them in a list \n",
    "\n",
    "    - Get the synopsis and store them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The story of Henry Hill and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.',\n",
       " 'A young F.B.I. cadet must receive the help of an incarcerated and manipulative cannibal killer to help catch another serial killer, a madman who skins his victims.',\n",
       " 'When a simple jewelry heist goes horribly wrong, the surviving criminals begin to suspect that one of them is a police informant.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_parsed = []\n",
    "titles = []\n",
    "synopsis = []\n",
    "\n",
    "for i in range(len(pages)):\n",
    "    # parse all pages\n",
    "    pages_parsed.append(BeautifulSoup(pages[i].content, \"html.parser\"))\n",
    "    # select only the info about the movies\n",
    "    movies_html = pages_parsed[i].select(\"div.lister-item-content\")\n",
    "    # for movie, store titles and reviews into lists\n",
    "    for j in range(len(movies_html)):\n",
    "        titles.append(movies_html[j].select(\"h3 > a\")[0].get_text())\n",
    "        synopsis.append(movies_html[j].select(\"p:nth-child(4)\")[0].get_text().strip())\n",
    "\n",
    "\n",
    "# Checking our output:\n",
    "print(len(titles)) # output: 631\n",
    "print(len(synopsis))  # output: 631\n",
    "\n",
    "# Note: in your output the movie titles might be in English:\n",
    "titles[0:3] # output: ['El silencio de los corderos', 'Uno de los nuestros', 'Solo en casa']\n",
    "synopsis[0:3] #output: ['A young F.B.I. cadet must receive the help of an incarcerated and manipulative cannibal killer to help catch another serial killer, a madman who skins his victims.', 'The story of Henry Hill and his life in the mob, covering his relationship with his wife Karen Hill and his mob partners Jimmy Conway and Tommy DeVito in the Italian-American crime syndicate.', 'An eight-year-old troublemaker must protect his house from a pair of burglars when he is accidentally left home alone by his family during Christmas vacation.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping presidents\n",
    "\n",
    "Our objective is to create a dataframe with information about the presidents of the United States. To do this, we will go through this steps:\n",
    "\n",
    "1. Scrape this [list of presidents of the United States](https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 2. find url and store it in a variable\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States\"\n",
    "\n",
    "# 3. download html with a get request\n",
    "response = requests.get(url)\n",
    "response.status_code # 200 status code means OK!\n",
    "\n",
    "# 4.1. parse html (create the 'soup')\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "# 4.2. check that the html code looks like it should\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/wiki/George_Washington\" title=\"George Washington\">George Washington</a>,\n",
       " <a href=\"/wiki/John_Adams\" title=\"John Adams\">John Adams</a>,\n",
       " <a href=\"/wiki/Thomas_Jefferson\" title=\"Thomas Jefferson\">Thomas Jefferson</a>,\n",
       " <a href=\"/wiki/James_Madison\" title=\"James Madison\">James Madison</a>,\n",
       " <a href=\"/wiki/James_Monroe\" title=\"James Monroe\">James Monroe</a>,\n",
       " <a href=\"/wiki/John_Quincy_Adams\" title=\"John Quincy Adams\">John Quincy Adams</a>,\n",
       " <a href=\"/wiki/Andrew_Jackson\" title=\"Andrew Jackson\">Andrew Jackson</a>,\n",
       " <a href=\"/wiki/Martin_Van_Buren\" title=\"Martin Van Buren\">Martin Van Buren</a>,\n",
       " <a href=\"/wiki/William_Henry_Harrison\" title=\"William Henry Harrison\">William Henry Harrison</a>,\n",
       " <a href=\"/wiki/John_Tyler\" title=\"John Tyler\">John Tyler</a>,\n",
       " <a href=\"/wiki/James_K._Polk\" title=\"James K. Polk\">James K. Polk</a>,\n",
       " <a href=\"/wiki/Zachary_Taylor\" title=\"Zachary Taylor\">Zachary Taylor</a>,\n",
       " <a href=\"/wiki/Millard_Fillmore\" title=\"Millard Fillmore\">Millard Fillmore</a>,\n",
       " <a href=\"/wiki/Franklin_Pierce\" title=\"Franklin Pierce\">Franklin Pierce</a>,\n",
       " <a href=\"/wiki/James_Buchanan\" title=\"James Buchanan\">James Buchanan</a>,\n",
       " <a href=\"/wiki/Abraham_Lincoln\" title=\"Abraham Lincoln\">Abraham Lincoln</a>,\n",
       " <a href=\"/wiki/Andrew_Johnson\" title=\"Andrew Johnson\">Andrew Johnson</a>,\n",
       " <a href=\"/wiki/Ulysses_S._Grant\" title=\"Ulysses S. Grant\">Ulysses S. Grant</a>,\n",
       " <a href=\"/wiki/Rutherford_B._Hayes\" title=\"Rutherford B. Hayes\">Rutherford B. Hayes</a>,\n",
       " <a href=\"/wiki/James_A._Garfield\" title=\"James A. Garfield\">James A. Garfield</a>,\n",
       " <a href=\"/wiki/Chester_A._Arthur\" title=\"Chester A. Arthur\">Chester A. Arthur</a>,\n",
       " <a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>,\n",
       " <a href=\"/wiki/Benjamin_Harrison\" title=\"Benjamin Harrison\">Benjamin Harrison</a>,\n",
       " <a href=\"/wiki/Grover_Cleveland\" title=\"Grover Cleveland\">Grover Cleveland</a>,\n",
       " <a href=\"/wiki/William_McKinley\" title=\"William McKinley\">William McKinley</a>,\n",
       " <a href=\"/wiki/Theodore_Roosevelt\" title=\"Theodore Roosevelt\">Theodore Roosevelt</a>,\n",
       " <a href=\"/wiki/William_Howard_Taft\" title=\"William Howard Taft\">William Howard Taft</a>,\n",
       " <a href=\"/wiki/Woodrow_Wilson\" title=\"Woodrow Wilson\">Woodrow Wilson</a>,\n",
       " <a href=\"/wiki/Warren_G._Harding\" title=\"Warren G. Harding\">Warren G. Harding</a>,\n",
       " <a href=\"/wiki/Calvin_Coolidge\" title=\"Calvin Coolidge\">Calvin Coolidge</a>,\n",
       " <a href=\"/wiki/Herbert_Hoover\" title=\"Herbert Hoover\">Herbert Hoover</a>,\n",
       " <a href=\"/wiki/Franklin_D._Roosevelt\" title=\"Franklin D. Roosevelt\">Franklin D. Roosevelt</a>,\n",
       " <a href=\"/wiki/Harry_S._Truman\" title=\"Harry S. Truman\">Harry S. Truman</a>,\n",
       " <a href=\"/wiki/Dwight_D._Eisenhower\" title=\"Dwight D. Eisenhower\">Dwight D. Eisenhower</a>,\n",
       " <a href=\"/wiki/John_F._Kennedy\" title=\"John F. Kennedy\">John F. Kennedy</a>,\n",
       " <a href=\"/wiki/Lyndon_B._Johnson\" title=\"Lyndon B. Johnson\">Lyndon B. Johnson</a>,\n",
       " <a href=\"/wiki/Richard_Nixon\" title=\"Richard Nixon\">Richard Nixon</a>,\n",
       " <a href=\"/wiki/Gerald_Ford\" title=\"Gerald Ford\">Gerald Ford</a>,\n",
       " <a href=\"/wiki/Jimmy_Carter\" title=\"Jimmy Carter\">Jimmy Carter</a>,\n",
       " <a href=\"/wiki/Ronald_Reagan\" title=\"Ronald Reagan\">Ronald Reagan</a>,\n",
       " <a href=\"/wiki/George_H._W._Bush\" title=\"George H. W. Bush\">George H. W. Bush</a>,\n",
       " <a href=\"/wiki/Bill_Clinton\" title=\"Bill Clinton\">Bill Clinton</a>,\n",
       " <a href=\"/wiki/George_W._Bush\" title=\"George W. Bush\">George W. Bush</a>,\n",
       " <a href=\"/wiki/Barack_Obama\" title=\"Barack Obama\">Barack Obama</a>,\n",
       " <a href=\"/wiki/Donald_Trump\" title=\"Donald Trump\">Donald Trump</a>,\n",
       " <a href=\"/wiki/Joe_Biden\" title=\"Joe Biden\">Joe Biden</a>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this solution is not very elegant, but works. \n",
    "# The CSS selector we copied had an \"nth-child\" that we could iterate \n",
    "# to find presidents, but some elements were empty, so we concatenate \n",
    "# each new element with \"+\" instead of appending as usual:\n",
    "\n",
    "presidents = []\n",
    "\n",
    "for i in range(95):\n",
    "    presidents = presidents + soup.select(\"tbody > tr:nth-child(\" + str(i) + \") > td:nth-child(4) > b > a\")\n",
    "\n",
    "# check the output:\n",
    "presidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Collect all the links to the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wiki/George_Washington'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can access the links searching for the attribute \"href\"\n",
    "# in each element\n",
    "presidents[0][\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we just assemble a new request to the link\n",
    "# send request\n",
    "url = \"https://en.wikipedia.org/\" + presidents[0][\"href\"]\n",
    "response = requests.get(url)\n",
    "response.status_code\n",
    "\n",
    "# parse & store html\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "#soup.find(\"table\", {\"class\":\"infobox vcard\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scrape the Wikipedia page of each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we could very well store the whole wikipedia page for each president, or just the tiny, final pieces of information. Storing the boxes is a middle ground (we don't have too much noise but retain the flexibility of deciding later which specific elements to extract).\n",
    "\n",
    "When sending multiple requests, remember to be respectful by spacing the requests a few seconds from each other. We will also pring the success code to monitor that everything is going well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "George Washington 200\n",
      "I will sleep for 2 second/s.\n",
      "John Adams 200\n",
      "I will sleep for 1 second/s.\n",
      "Thomas Jefferson 200\n",
      "I will sleep for 2 second/s.\n",
      "James Madison 200\n",
      "I will sleep for 1 second/s.\n",
      "James Monroe 200\n",
      "I will sleep for 2 second/s.\n",
      "John Quincy Adams 200\n",
      "I will sleep for 1 second/s.\n",
      "Andrew Jackson 200\n",
      "I will sleep for 2 second/s.\n",
      "Martin Van Buren 200\n",
      "I will sleep for 1 second/s.\n",
      "William Henry Harrison 200\n",
      "I will sleep for 1 second/s.\n",
      "John Tyler 200\n",
      "I will sleep for 2 second/s.\n",
      "James K. Polk 200\n",
      "I will sleep for 1 second/s.\n",
      "Zachary Taylor 200\n",
      "I will sleep for 2 second/s.\n",
      "Millard Fillmore 200\n",
      "I will sleep for 2 second/s.\n",
      "Franklin Pierce 200\n",
      "I will sleep for 2 second/s.\n",
      "James Buchanan 200\n",
      "I will sleep for 1 second/s.\n",
      "Abraham Lincoln 200\n",
      "I will sleep for 2 second/s.\n",
      "Andrew Johnson 200\n",
      "I will sleep for 1 second/s.\n",
      "Ulysses S. Grant 200\n",
      "I will sleep for 1 second/s.\n",
      "Rutherford B. Hayes 200\n",
      "I will sleep for 2 second/s.\n",
      "James A. Garfield 200\n",
      "I will sleep for 1 second/s.\n",
      "Chester A. Arthur 200\n",
      "I will sleep for 2 second/s.\n",
      "Grover Cleveland 200\n",
      "I will sleep for 2 second/s.\n",
      "Benjamin Harrison 200\n",
      "I will sleep for 2 second/s.\n",
      "Grover Cleveland 200\n",
      "I will sleep for 1 second/s.\n",
      "William McKinley 200\n",
      "I will sleep for 1 second/s.\n",
      "Theodore Roosevelt 200\n",
      "I will sleep for 2 second/s.\n",
      "William Howard Taft 200\n",
      "I will sleep for 1 second/s.\n",
      "Woodrow Wilson 200\n",
      "I will sleep for 1 second/s.\n",
      "Warren G. Harding 200\n",
      "I will sleep for 2 second/s.\n",
      "Calvin Coolidge 200\n",
      "I will sleep for 2 second/s.\n",
      "Herbert Hoover 200\n",
      "I will sleep for 2 second/s.\n",
      "Franklin D. Roosevelt 200\n",
      "I will sleep for 2 second/s.\n",
      "Harry S. Truman 200\n",
      "I will sleep for 1 second/s.\n",
      "Dwight D. Eisenhower 200\n",
      "I will sleep for 2 second/s.\n",
      "John F. Kennedy 200\n",
      "I will sleep for 1 second/s.\n",
      "Lyndon B. Johnson 200\n",
      "I will sleep for 2 second/s.\n",
      "Richard Nixon 200\n",
      "I will sleep for 2 second/s.\n",
      "Gerald Ford 200\n",
      "I will sleep for 1 second/s.\n",
      "Jimmy Carter 200\n",
      "I will sleep for 1 second/s.\n",
      "Ronald Reagan 200\n",
      "I will sleep for 1 second/s.\n",
      "George H. W. Bush 200\n",
      "I will sleep for 1 second/s.\n",
      "Bill Clinton 200\n",
      "I will sleep for 1 second/s.\n",
      "George W. Bush 200\n",
      "I will sleep for 2 second/s.\n",
      "Barack Obama 200\n",
      "I will sleep for 2 second/s.\n",
      "Donald Trump 200\n",
      "I will sleep for 1 second/s.\n",
      "Joe Biden 200\n",
      "I will sleep for 1 second/s.\n"
     ]
    }
   ],
   "source": [
    "# 2. find url and store it in a variable\n",
    "\n",
    "presi_soups = []\n",
    "\n",
    "for presi in presidents:\n",
    "    # send request\n",
    "    url = \"https://en.wikipedia.org/\" + presi[\"href\"]\n",
    "    response = requests.get(url)\n",
    "    print(presi.get_text(), response.status_code)\n",
    "    \n",
    "    # parse & store html\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    presi_soups.append(soup.find(\"table\", {\"class\":\"infobox vcard\"}))\n",
    "    \n",
    "    # respectful nap:\n",
    "    wait_time = randint(1,2)\n",
    "    print(\"I will sleep for \" + str(wait_time) + \" second/s.\")\n",
    "    sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Find and store information about each president.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted the 'infoboxes': now it's time to exctract especific pieces of information from them. Let's test what can we get from single presidents and then assemble a loop for all of them - as usual.\n",
    "\n",
    "Here, we will use [the string argument](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#the-string-argument) in the find function, since wikipedia tags and classes are not always helpfulto locate. The string argument allows us to locate elements by its actual content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Birthday\n",
    "presi_soups[-1].find(\"span\", {\"class\":\"bday\"}).get_text()\n",
    "\n",
    "#Political party\n",
    "presi_soups[-1].find(\"th\", string=\"Political party\").parent.find(\"a\").get_text()\n",
    "\n",
    "#Number of sons/daughters\n",
    "len(presi_soups[-1].find(\"th\", string=\"Children\").parent.find_all(\"li\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Organize the information in a dataframe where we have each president as a row and each variable we collected as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
